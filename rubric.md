# Trial Project Assessment Rubric

Each category is scored from **0 (Missing/Unsatisfactory)** to **5 (Outstanding)**.  
Maximum possible score: **45**.

| Category | Description | Score (0–5) |
|---------|-------------|-------------|
| **Context Handling** | Uses provided personal documents effectively and references them accurately in responses. | |
| **Agentic Thinking** | Demonstrates personality, multiple modes, or interactive features beyond basic Q&A. | |
| **Use of Personal Data** | Dataset is thoughtful, authentic, and representative of the candidate. | |
| **Build Quality** | Code is runnable, documented, secure, and maintainable; behavior is verifiable. | |
| **Voice & Reflection** | Responses sound like the candidate and give insight into unique traits or experiences. | |
| **Bonus Effort** | Stretch features, creative polish, or pleasant surprises. | |
| **AI Build Artifacts** | Includes prompt histories, agent instructions, or other evidence of AI collaboration. | |
| **RAG Usage (Optional)** | Effective use of retrieval‑augmented generation or clear plan to implement it. | |
| **Submission Completeness** | GitHub repo, deployed agent, short video walkthrough, and “Show Your Thinking” materials. | |

---

## Scoring Guidelines

### Context Handling
- **0:** No use of personal documents or inaccurate references  
- **1–2:** Partial references, context errors, or missing data  
- **3–4:** Correctly references most documents, minor omissions  
- **5:** Comprehensive, accurate, context-aware across all materials

### Agentic Thinking
- **0:** Simple Q&A with no personality or modes  
- **1–2:** Limited interactivity; personality not evident  
- **3–4:** Multiple modes or distinct tone choices; some personality  
- **5:** Strong personality, nuanced modes, or interactive behaviors

### Use of Personal Data
- **0:** No personal data beyond CV  
- **1–2:** Minimal or generic supporting documents  
- **3–4:** Varied and relevant materials reflecting the candidate  
- **5:** Rich, well-organized dataset offering depth and authenticity

### Build Quality
- **0:** App cannot run; documentation absent  
- **1–2:** Runs with errors or poor documentation; insecure practices  
- **3–4:** Works with minor issues; clear instructions; basic tests  
- **5:** Robust, secure, well-documented, and easily testable

### Voice & Reflection
- **0:** Responses generic or inconsistent  
- **1–2:** Occasional personal tone but mostly generic  
- **3–4:** Consistent voice that reflects the candidate  
- **5:** Engaging, reflective voice conveying unique perspective

### Bonus Effort
- **0:** No additional features  
- **1–2:** Minor enhancements or polish  
- **3–4:** One or two thoughtful optional features (e.g., mode switcher)  
- **5:** Multiple creative or polished additions that elevate the project

### AI Build Artifacts
- **0:** No artifacts or evidence of AI collaboration  
- **1–2:** Minimal mention of AI usage  
- **3–4:** Clear examples of prompts or agent instructions  
- **5:** Comprehensive artifacts showing iterative collaboration

### RAG Usage (Optional)
- **0:** No RAG and no explanation  
- **1–2:** RAG present but poorly executed or undocumented  
- **3–4:** Effective RAG implementation or clear future plan  
- **5:** Robust RAG with explanation of design choices and evaluation

### Submission Completeness
- **0:** Missing multiple required deliverables  
- **1–2:** Some deliverables provided but incomplete  
- **3–4:** All deliverables present with minor gaps  
- **5:** All deliverables complete, well-presented, and easy to access